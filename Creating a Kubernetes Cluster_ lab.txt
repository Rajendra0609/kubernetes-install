Creating a Kubernetes Cluster
Introduction
In this hands-on lab, we will install and configure a Kubernetes cluster consisting of 1 master and 2 nodes. Once the installation and configuration are complete, we will have a 3-node Kubernetes cluster that uses Flannel as the network overlay.

Logging In
Use the credentials provided on the hands-on lab overview page to log into the master and server nodes as cloud_user. It's probably a good idea to have three terminals open, one for each node.

Install containerd and Kubernetes on All Servers
Most commands need to be run on all nodes. Pay attention though. Commands to initialize and configure the cluster need to be run only on the master node, whereas kubeadm join commands need to be run only on node1 and node2. There are notes down there, just watch for them.

Once we have logged in, we need to elevate privileges using sudo:

sudo su  
Create the configuration file for containerd:

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
Load the modules:

sudo modprobe overlay
sudo modprobe br_netfilter
Set the system configurations for Kubernetes networking:

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
Apply the new settings:

sudo sysctl --system
Add the stable Docker Community Edition repository to yum:

yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
Install containerd:

sudo yum install -y containerd.io
Create the default configuration folder for containerd:

sudo mkdir -p /etc/containerd
Generate the default containerd configuration, and save it to the newly created default file:

sudo containerd config default | sudo tee /etc/containerd/config.toml
Restart containerd to ensure the new configuration file is used:

sudo systemctl restart containerd
Verify that containerd is running:

sudo systemctl status containerd
Disable swap:

sudo swapoff -a
Set SELinux in permissive mode (effectively disabling it):

 sudo setenforce 0
 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
Add the Kubernetes repository (this overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repo):

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
Install kubelet, kubeadm and kubectl:

sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
Enable the kubelet service before running kubeadm:

sudo systemctl enable --now kubelet
Note: Complete the following section on the MASTER ONLY!
Initialize the cluster using the IP range for Flannel:

kubeadm init --pod-network-cidr=10.244.0.0/16
Copy the kubeadmn join command that is in the output. We will need this later.

Exit sudo, copy the admin.conf to your home directory, and take ownership:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Deploy Flannel:

kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
Check the cluster state:

kubectl get pods --all-namespaces
Note: Complete the following steps on the NODES ONLY!
Run the join command that you copied earlier, this requires running the command prefaced with sudo on the nodes (if we hadn't run sudo su to begin with). Then we'll check the nodes from the master.

kubectl get nodes
Create and Scale a Deployment Using kubectl
Note: These commands will only be run on the master node.
Create a simple deployment:

kubectl create deployment nginx --image=nginx
Inspect the pod:

kubectl get pods
Scale the deployment:

kubectl scale deployment nginx --replicas=4
Inspect the pods. We should have four now:

kubectl get pods
Conclusion
We got it done. We've created Kubernetes cluster, and deployed Nginx on it. Then we scaled up to four pods. Congratulations!